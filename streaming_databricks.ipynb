{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c15ffa6-9f7e-47f0-98b1-f1304a303e89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc74059-35c8-4aef-9584-a80fc9bd4689",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading of the data is done in these code blocks. Each of the three streams are read into three separate tables, using the credentials from the code blocks above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953d2e2e-2008-4e53-a4ed-dbe522bf70b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_pin_df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0e90e0175553-pin') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_user_df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0e90e0175553-user') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_geo_df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0e90e0175553-geo') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the data from the stream, we cast the data column as a string and create a schema for Spark to be able to label the data in a table form. Then the table is displayed. Because each table contains different data, we create the correct schema for each table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98c8171-2778-48c2-8124-ae4ab8766e2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_pin_df = streaming_pin_df.selectExpr(\"CAST(data as STRING) AS jsonData\") \n",
    "# display(pindf) \n",
    "schema = StructType([ \n",
    "                     StructField(\"index\", StringType(), True), \n",
    "                     StructField(\"unique_id\", StringType(), True), \n",
    "                     StructField(\"title\", StringType(), True), \n",
    "                     StructField(\"description\", StringType(), True), \n",
    "                     StructField(\"follower_count\", StringType(), True),\n",
    "                     StructField(\"poster_name\", StringType(), True), \n",
    "                     StructField(\"tag_list\", StringType(), True), \n",
    "                     StructField(\"is_image_or_video\", StringType(), True), \n",
    "                     StructField(\"image_src\", StringType(), True),\n",
    "                     StructField(\"save_location\", StringType(), True), \n",
    "                     StructField(\"category\", StringType(), True), ])\n",
    "\n",
    "streaming_pin_df = streaming_pin_df.select(from_json('jsonData',schema) \\\n",
    "     .alias(\"data\")) \\\n",
    "     .select(\"data.*\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_user_df = streaming_user_df.selectExpr(\"CAST(data as STRING) AS jsonData\")\n",
    "\n",
    "schema = StructType([ \n",
    "                     StructField(\"ind\", StringType(), True), \n",
    "                     StructField(\"first_name\", StringType(), True), \n",
    "                     StructField(\"last_name\", StringType(), True), \n",
    "                     StructField(\"age\", StringType(), True), \n",
    "                     StructField(\"date_joined\", StringType(), True) ])\n",
    "\n",
    "streaming_user_df = streaming_user_df.select(from_json('jsonData',schema) \\\n",
    "     .alias(\"data\")) \\\n",
    "     .select(\"data.*\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_geo_df = streaming_geo_df.selectExpr(\"CAST(data as STRING) AS jsonData\") \n",
    " \n",
    "schema = StructType([ \n",
    "                     StructField(\"ind\", StringType(), True), \n",
    "                     StructField(\"timestamp\", StringType(), True), \n",
    "                     StructField(\"latitude\", StringType(), True), \n",
    "                     StructField(\"longitude\", StringType(), True), \n",
    "                     StructField(\"country\", StringType(), True) ])\n",
    "\n",
    "streaming_geo_df = streaming_geo_df.select(from_json('jsonData',schema) \\\n",
    "     .alias(\"data\")) \\\n",
    "     .select(\"data.*\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we begin performing the data cleaning of each table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f0a9fc-8bf8-4221-a2d5-96dfc6bceeb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pin = streaming_pin_df.select('*')\n",
    "\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind') \n",
    "df_pin= df_pin.withColumn( 'save_location', regexp_replace('save_location', 'Local save in ', ''))\n",
    "df_pin= df_pin.withColumn('follower_count', regexp_replace('follower_count', 'k', '000'))\n",
    "df_pin= df_pin.withColumn('follower_count', regexp_replace('follower_count', 'M', '000000'))\n",
    "df_pin= df_pin.withColumn('follower_count', col('follower_count').cast(\"int\"))\n",
    "df_pin= df_pin.withColumn('ind', col('ind').cast(\"int\"))\n",
    "df_pin = df_pin.select([when(col(c)==\"No description available Story format\",None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "df_pin = df_pin.select([when(col(c)==\"No description available\",None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "df_pin = df_pin.select([when(col(c)==\"Untitled\",None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "df_pin = df_pin.select([when(col(c)==\"No Title Data Available\",None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "df_pin = df_pin.select([when(col(c)==\"User Info Error\",None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "df_pin = df_pin.select([when(col(c)==\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\",None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "df_pin = df_pin.select([when(col(c)==\"Image src error.\",None).otherwise(col(c)).alias(c) for c in df_pin.columns])\n",
    "df_pin = df_pin.select('ind', 'unique_id', 'title','description', 'follower_count', 'poster_name', 'tag_list', 'is_image_or_video', 'image_src', 'save_location',  'category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560a36bf-10a3-4582-aba7-5a6ad9237ba1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_user = streaming_user_df.select('*')\n",
    "  \n",
    "df_user= df_user.withColumn( 'user_name', concat( 'first_name', lit(' '),'last_name'))\n",
    "df_user = df_user.drop('first_name') \n",
    "df_user = df_user.drop('last_name') \n",
    "df_user= df_user.withColumn('date_joined', col('date_joined').cast(\"timestamp\"))\n",
    "df_user= df_user.withColumn('age', col('age').cast(\"int\"))\n",
    "df_user = df_user.select('ind', 'user_name', 'age','date_joined') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df8a60f-7687-471d-a282-6288d0edec14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_geo = streaming_geo_df.select('*')\n",
    "  \n",
    "df_geo= df_geo.withColumn('coordinates', struct(df_geo.latitude, df_geo.longitude))\n",
    "df_geo = df_geo.drop('latitude') \n",
    "df_geo = df_geo.drop('longitude') \n",
    "df_geo= df_geo.withColumn('timestamp', col('timestamp').cast(\"timestamp\"))\n",
    "df_geo = df_geo.select('ind', 'country', 'coordinates','timestamp') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, the cleaned tables can now be sent as a stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f682bae5-f82e-4403-aef0-09c7a8606c33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[125]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f4051ba7400&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[125]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f4051ba7400&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pin.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0e90e0175553_pin_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb85946-9628-4d45-9486-d38770b73ac5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[110]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f40519c3d00&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[110]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f40519c3d00&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_user.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0e90e0175553_user_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc294d9-b386-47dd-a3ca-e5e634c83d2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[111]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f40479db0a0&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[111]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f40479db0a0&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_geo.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0e90e0175553_geo_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-12-11 18:33:47",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
